{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b536537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAL CRITERIA EVAL BENCHMARK\n",
    "\n",
    "from src.metrics import Metrics\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tabulate import tabulate\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "model = os.getenv(\"MODEL\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "metrics = Metrics(model=model, api_key=api_key)\n",
    "\n",
    "df = pd.read_csv(\"eval_benchmark.csv\")\n",
    "\n",
    "results = {\n",
    "    \"true_positives\": [],\n",
    "    \"false_positives\": [],\n",
    "    \"true_negatives\": [],\n",
    "    \"false_negatives\": []\n",
    "}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    full_result = metrics.criteria_eval(content=row[\"assistant\"],criteria=[row[\"criterion\"]])\n",
    "    result = full_result[\"results\"][0][\"result\"]\n",
    "\n",
    "    expected = row[\"satisfies\"]\n",
    "\n",
    "    if result is True:\n",
    "        if result == expected:\n",
    "            results[\"true_positives\"].append(row)\n",
    "        else:\n",
    "            results[\"false_positives\"].append(row)\n",
    "    else:\n",
    "        if result == expected:\n",
    "            results[\"true_negatives\"].append(row)\n",
    "        else:\n",
    "            results[\"false_negatives\"].append(row)\n",
    "    print(f\"Assistant: {row['assistant']} | Criterion: {row['criterion']} | Result: {result} | Expected: {expected}\")\n",
    "\n",
    "tp = len(results[\"true_positives\"])\n",
    "fp = len(results[\"false_positives\"])\n",
    "tn = len(results[\"true_negatives\"])\n",
    "fn = len(results[\"false_negatives\"])\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "table = [\n",
    "    [\"True Positives\", tp],\n",
    "    [\"False Positives\", fp],\n",
    "    [\"True Negatives\", tn],\n",
    "    [\"False Negatives\", fn],\n",
    "    [\"Precision\", f\"{precision:.2f}\"],\n",
    "    [\"Recall\",    f\"{recall:.2f}\"],\n",
    "    [\"F1 Score\",  f\"{f1:.2f}\"],\n",
    "]\n",
    "print(tabulate(table, headers=[\"Metric\",\"Value\"], tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d64c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLAIM CHECK EXAMPLE\n",
    "\n",
    "from src.data_sources import DataSource\n",
    "from src.metrics import Metrics\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "model = os.getenv(\"MODEL\")\n",
    "api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "metrics = Metrics(model=model, api_key=api_key)\n",
    "\n",
    "with open(\"test_content.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_content = f.read()\n",
    "\n",
    "result = metrics.claim_check(\n",
    "    content=test_content,\n",
    "    data_source=DataSource.WEB,\n",
    "    urls=[\n",
    "        \"https://www.cdc.gov/diabetes/healthy-eating/diabetes-meal-planning.html\",\n",
    "        \"https://www.cdc.gov/diabetes/hcp/clinical-guidance/index.html\",\n",
    "        \"https://www.who.int/news-room/fact-sheets/detail/diabetes\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
